\subsubsection{Convolutional Neural Networks}

\begin{definition} \hlt{Convolutional Neural Networks (CNNs)}\\
Neural network for processing data with a known, grid-like topology. Uses convolution instead of general matrix multiplication in at least one of their layers.
\end{definition}

\begin{definition} \hlt{Convolution}\\
The convolution of two functions $f$ and $g$, denoted by $\ast$, is the integral of product of two functions after one is reflected about the y-axis and shifted.
\begin{align}
(f \ast g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) d \tau \nonumber
\end{align} 
The first argument $f(\tau)$ is the input, and second argument $g(t - \tau)$ is the kernel. The output is feature map.
\end{definition}

\begin{definition} \hlt{Discrete Convolution}\\
For complex-valued functions $f$ and $g$ defined on set of $\Z$ integers, the discrete convolution of $f$ and $g$ is
\begin{align}
(f \ast g)[n] = \sum\limits_{m=-\infty}^{\infty} f[m] g[n-m] \nonumber
\end{align}
If function $g_N$ is periodic with period $N$, then the convolution is also periodic and identical to
\begin{align}
(f \ast g_N)[n] = \sum\limits_{m=0}^{N-1} \left( \sum\limits_{k=-\infty}^{\infty} f[m+kN] \right) g_N [n-m] \nonumber
\end{align}
\end{definition}

Note that convolution is commutative, as the kernel is flipped relative to the input (as $m$ increases, the index into input increases, but index into kernel decreases), but this is not an important property of neural network implementation. Neural network libraries implement cross-correlation instead.

\begin{definition} \hlt{Cross-Correlation}\\
A measure of similarity of two series as a function of the displacement of one relative to the other. Also known as \hlt{sliding inner or dot product}. This is defined as
\begin{align}
(f \star g)(\tau) = \int_{-\infty}^{\infty} \overline{f(\tau)}g(t + \tau) dt \nonumber
\end{align}
This is related to convolution by
\begin{align}
[f(t) \star g{t}](t) = [\overline{f(-t)} \ast g(t)] (t) \nonumber
\end{align}
\end{definition}

\begin{remark} \hlt{Motivation: Sparse Interactions}\\
The kernel is smaller than input, but is able to detect meaningful features. Hence fewer parameters are to be stored, reducing memory requirements of the model. Units in deeper layers may indirectly interact with a larger portion of the input, hence efficiently describe complicated interactions between many variables.
\end{remark}

\begin{remark} \hlt{Motivation: Parameter Sharing}\\
Each member of kernel is used at every position of the input except at boundary, which depends on design decisions regarding the boundary. Only one set of parameter is required to be learned. The runtime of forward propagation is at $O(k \times n)$ for $k \lll m$ connections for each input, given $m$ input and $n$ outputs.
\end{remark}

\begin{remark} \hlt{Motivation: Equivariant Representations}\\
An \hlt{equivariant} function is such that if the input changes, the output changes in the same way.\\
Applying a transformation, then convolution, is equivalent to applying convolution, then the transformation.
\end{remark}

\begin{remark} \hlt{Typical CNN Layers}\\
The layers of a typical CNN are:
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Convolution: layer performs several convolutions in parallel to produce set of linear activations
\item Detector: each linear activation is run through a non-linear activation function
\item Pooling: modify the output of the layer further with summary statistics of nearby outputs
\end{enumerate}
\end{remark}

\begin{example} \hlt{Example Types of Pooling}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Max Pooling: reports maximum output for each region of the feature map
\item Average Pooling: reports average for each region of the feature map
\item $L^2$ Pooling: calculates Euclidean norm ($L^2$ norm) for each region of the feature map
\end{enumerate}
\end{example}

\begin{remark} \hlt{Pooling on Invariance}\\
Pooling makes the representation approximately invariant to small translations of input.\\
Use of pooling is analogous to adding an infinitely strong prior that the function the layer learns must be invariant to small translations. As pooling summarises responses over whole neighbourhood, fewer pooling units than detector units may be used. To handle inputs of varying size, the size of offset between pooling regions may be changed so that the classification layer always receives the same number of summary statistics.
\end{remark}

\begin{remark} \hlt{Multi-Channel Operations}\\
Not commutative unless each operation has the same number of output and input channels.\\
Assume there exists $4$-D kernel tensor $\mathcal{K}$ with element $K_{i,j,k,l}$ giving connection between strength between a unit in channel $i$ of the output and a unit in channel $j$ of the input, with offset of $k$ rows and $l$ columns between the output unit and input unit. Assume input consists of observed data $\mathcal{V}$ with element $V_{i,j,k}$ giving the value of input unit within channel $i$ at row $j$ and column $k$. Assume output consists of $\mathcal{Z}$ in same format as $\mathcal{V}$. If $\mathbb{Z}$ is produced by convolving $\mathcal{K}$ across $\mathcal{V}$ without flipping $\mathcal{K}$, then
\begin{align}
Z_{i,j,k} = \sum\limits_{l, m, n} V_{l, j+m-1, k+n-1} K_{i,l,m,n} \nonumber
\end{align}
where summation over $l,m,n$ is over all values for which tensor indexing operations inside summation is valid.
\end{remark}

\begin{remark} \hlt{Downsampled Convolution}\\
To reduce computational cost by sampling only every $s$ pixels (\hlt{stride}) in each direction of output, then
\begin{align}
Z_{i,j,k} = c(\mathcal{K}, \mathcal{V}, s)_{i,j,k} = \sum\limits_{l,m,n} [\mathcal{V}_{l, (j-1) \times s + m, (k-1) \times s + n} \mathcal{K}_{i,l,m,n}] \nonumber
\end{align}
\end{remark}

\begin{remark} \hlt{Zero Padding}\\
Zero-padding is adding zeroes to end of input sequence $\mathcal{V}$ so that the total number of samples is equal to the next higher multiple of two.
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Valid Convolution: no zero-padding used. If input image has width $m$ and kernel has width $k$, the output will be width $m-k+1$. Rate of shrinkage can be dramatic if kernel used are large.
\item Same Convolution: just enough zero-padding is added to keep size of output equal to size of input. Border pixels may be underrepresented in the model.
\item Full Convolution: zeroes are added for every pixel to be visited $k$ times in each direction, resulting in an input image of width $m+k-1$.
\end{enumerate}
\end{remark}

\begin{remark} \hlt{Locally Connected Layers (Unshared Convolution)}\\
Similar operation to discrete convolution with small kernel, but without parameter sharing. Adjacency matrix of MLP is the same, but every connection has its own weight as specified by $6$-D tensor $\mathcal{W}$, where the indices of $\mathcal{W}$ are the output channel $i$, the output row $j$, the output column $k$, the input channel $l$, the row offset within the the input $m$, and the column offset within the input $n$. The liner part of the layer is then
\begin{align}
Z_{i,j,k} = \sum\limits_{l,m,n} [\mathcal{V}_{l, j+m-1, k+n-1} w_{i,j,k,l,m,n}] \nonumber
\end{align}
Locally connected layers are useful when it is known that each feature should be function of a small part of space but would not occur across all of space.
\end{remark}

\begin{remark} \hlt{Tiled Convolution}\\
Compromise between convolutional layer and locally connected layer. A set of kernels will be rotated through space. Immediate neighbouring locations will have different filters, but memory requirements for storing the parameters will increase only by a factor of the size of this set of kernels.\\
Let $k$ be a $6$-D tensor, where two of the dimensions correspond to different locations in output matrix. Output locations cycle through a set of $t$ different choices of kernel stack in each direction.
\begin{align}
Z_{i,j,k} = \sum\limits_{l,m,n} \mathcal{V}_{l, j+m-1, k+n-1} \mathcal{K}_{i,l,m,n, j \text{ mod } t + 1, k \text{ mod } t + 1} \nonumber
\end{align}  
\end{remark}


