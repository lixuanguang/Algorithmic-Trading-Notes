\subsubsection{Convolutional Neural Networks}

\begin{definition} \hlt{Convolutional Neural Networks (CNNs)}\\
Neural network for processing data with a known, grid-like topology. Uses convolution instead of general matrix multiplication in at least one of their layers.
\end{definition}

\begin{definition} \hlt{Convolution}\\
The convolution of two functions $f$ and $g$, denoted by $\ast$, is the integral of product of two functions after one is reflected about the y-axis and shifted.
\begin{align}
(f \ast g)(t) = \int_{-\infty}^{\infty} f(\tau)g(t - \tau) d \tau \nonumber
\end{align} 
The first argument $f(\tau)$ is the input, and second argument $g(t - \tau)$ is the kernel. The output is feature map.
\end{definition}

\begin{definition} \hlt{Discrete Convolution}\\
For complex-valued functions $f$ and $g$ defined on set of $\Z$ integers, the discrete convolution of $f$ and $g$ is
\begin{align}
(f \ast g)[n] = \sum\limits_{m=-\infty}^{\infty} f[m] g[n-m] \nonumber
\end{align}
If function $g_N$ is periodic with period $N$, then the convolution is also periodic and identical to
\begin{align}
(f \ast g_N)[n] = \sum\limits_{m=0}^{N-1} \left( \sum\limits_{k=-\infty}^{\infty} f[m+kN] \right) g_N [n-m] \nonumber
\end{align}
\end{definition}

Note that convolution is commutative, as the kernel is flipped relative to the input (as $m$ increases, the index into input increases, but index into kernel decreases), but this is not an important property of neural network implementation. Neural network libraries implement cross-correlation instead.

\begin{definition} \hlt{Cross-Correlation}\\
A measure of similarity of two series as a function of the displacement of one relative to the other. Also known as \hlt{sliding inner or dot product}. This is defined as
\begin{align}
(f \star g)(\tau) = \int_{-\infty}^{\infty} \overline{f(\tau)}g(t + \tau) dt \nonumber
\end{align}
This is related to convolution by
\begin{align}
[f(t) \star g{t}](t) = [\overline{f(-t)} \ast g(t)] (t) \nonumber
\end{align}
\end{definition}

\begin{remark} \hlt{Motivation: Sparse Interactions}\\
The kernel is smaller than input, but is able to detect meaningful features. Hence fewer parameters are to be stored, reducing memory requirements of the model. Units in deeper layers may indirectly interact with a larger portion of the input, hence efficiently describe complicated interactions between many variables.
\end{remark}

\begin{remark} \hlt{Motivation: Parameter Sharing}\\
Each member of kernel is used at every position of the input except at boundary, which depends on design decisions regarding the boundary. Only one set of parameter is required to be learned. The runtime of forward propagation is at $O(k \times n)$ for $k \lll m$ connections for each input, given $m$ input and $n$ outputs.
\end{remark}

\begin{remark} \hlt{Motivation: Equivariant Representations}\\
An \hlt{equivariant} function is such that if the input changes, the output changes in the same way.\\
Applying a transformation, then convolution, is equivalent to applying convolution, then the transformation.
\end{remark}

\begin{remark} \hlt{Typical CNN Layers}\\
The layers of a typical CNN are:
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Convolution: layer performs several convolutions in parallel to produce set of linear activations
\item Detector: each linear activation is run through a non-linear activation function
\item Pooling: modify the output of the layer further with summary statistics of nearby outputs
\end{enumerate}
\end{remark}

\begin{example} \hlt{Example Types of Pooling}
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Max Pooling: reports maximum output for each region of the feature map
\item Average Pooling: reports average for each region of the feature map
\item $L^2$ Pooling: calculates Euclidean norm ($L^2$ norm) for each region of the feature map
\end{enumerate}
\end{example}

