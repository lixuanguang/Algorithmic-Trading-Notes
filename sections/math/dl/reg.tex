\subsubsection{Regularisation for Deep Learning}

\begin{definition} \hlt{Regularisation} refers to adding a parameter norm penalty $\Omega(\bm{\theta})$ to the objective function $J$. The regularised objective function is then
\begin{equation}
\tilde{J}(\bm{\theta}; \bm{X}, \bm{y}) = J(\bm{\theta}; \bm{X}, \bm{y}) + \alpha \Omega(\bm{\theta}) \nonumber
\end{equation}
where $\alpha in [0, \infty)$ is a hyper-parameter that weights the contribution of norm penalty term.\\
For neural networks, the parameter norm penalty $\Omega$ is chosen such that it penalises only the weights of the affine transformation at each layer and leaves the biases un-regularised.
\end{definition}

\begin{definition} \hlt{$L^2$ (Ridge) Regularisation}\\
The regularisation term added to objective function is
\begin{equation}
\Omega(\bm{\theta}) = \frac{1}{2} \Vert \bm{w} \Vert^2_2 \nonumber
\end{equation}
\end{definition}

\begin{remark} \hlt{Behaviour of Weight Decay ($L^2$) Regularisation}\\
Assuming no bias parameter, a model have the following objective function and parameter gradient:
\begin{align}
\tilde{J}(\bm{w}; \bm{X}, \bm{y}) &= \frac{\alpha}{2} \bm{w}^T \bm{w} + J(\bm{w}; \bm{X}, \bm{y}) \nonumber \\
\nabla_{\bm{w}} \tilde{J}(\bm{w}; \bm{X}, \bm{y}) &= \alpha \bm{w} + \nabla_{\bm{w}} J(\bm{w}; \bm{X}, \bm{y}) \nonumber
\end{align}
On a single gradient step, the update is as follows:
\begin{align}
\bm{w} \leftarrow (1 - \epsilon \alpha)\bm{w} - \epsilon \nabla_{\bm{w}} J(\bm{w}; \bm{X}, \bm{y}) \nonumber
\end{align}
The addition of weight decay has modified learning rule to multiplicatively shrink weight vector by constant factor on each step before gradient update.\\
Using quadratic approximation to objective function at minimal unregularised training cost, approximation is
\begin{equation}
\hat{J}(\overline{\theta}) = J(\bm{w}^*) + \frac{1}{2} (\bm{w} - \bm{w}^*)\bm{H}(\bm{w} - \bm{w}^*), \ \ \ \ \bm{w}^* = \arg \min_{\bm{w}} J(\bm{w}) \nonumber
\end{equation}
where $\bm{H}$ is Hessian matrix of $J$ with respect to $\bm{w}$ evaluated at $\bm{w}^*$. Minimum of $\hat{J}$ occurs where gradient is
\begin{align}
\nabla_{\bm{w}} \hat{J}(\bm{w}) = \bm{H}(\bm{w} - \bm{w}^*) = \bm{0} \nonumber
\end{align}
Adding weight decay gradient, solve for minimum of regularised version of $\hat{J}$. Let $\tilde{\bm{w}}$ be minimum, then
\begin{align}
\alpha \tilde{\bm{w}} + \bm{H}(\tilde{\bm{w}} - \bm{w}^*) = 0 \nonumber \\
(\bm{H} + \alpha \bm{I})\tilde{\bm{w}} = \bm{H} \bm{w}^* \nonumber \\
\tilde{\bm{w}} = (\bm{H} + \alpha \bm{I})^{-1} \bm{H} \bm{w}^* \nonumber
\end{align}
As $\alpha \rightarrow 0$, regularised solution $\tilde{\bm{w}} \rightarrow \bm{w}^*$. Note $\bm{H}$ is real and symmetric, hence decompose into diagonal matrix $\bm{A}$ and orthonormal basis of eigenvectors $\bm{Q}$ such that $\bm{H} = \bm{Q} \bm{\Lambda} \bm{Q}^T$, to get
\begin{equation}
\tilde{\bm{w}} = (\bm{Q} \bm{\Lambda} \bm{Q}^T + \alpha \bm{I})^{-1} \bm{Q} \bm{\Lambda} \bm{Q}^T \bm{w}^* = \bm{Q} (\bm{\Lambda} + \alpha \bm{I})^{-1} \bm{\Lambda} \bm{Q}^T \bm{w}^* \nonumber
\end{equation}
The effect of weight decay rescale $\bm{w}^*$ along axes defined by eigenvectors of $\bm{H}$. The component of $\bm{w}^*$ aligned with $i$-th eigenvector of $\bm{H}$ is rescaled by factor $\frac{\lambda_i}{\lambda_i + \alpha}$. For components where $\lambda_i \gg \alpha$, the effects of regularisation is relatively small. For components where $\lambda_i \ll \alpha$, components will be shrunk to nearly zero magnitude.
\end{remark}

\begin{definition} \hlt{$L^1$ Regularisation}\\
The $L^1$ regularisation is the sum of absolute values of individual parameters.
\begin{equation}
\Omega(\bm{\theta}) = \Vert \bm{w} \Vert_1 = \sum_i \abs{w_i} \nonumber
\end{equation}
\end{definition}

\begin{definition} \hlt{Behaviour of $L^1$ Regularisation}\\
Assuming no bias parameter, a model has the following objective function and parameter gradient:
\begin{align}
\tilde{J}(\bm{w}; \bm{X}, \bm{y}) &= \alpha \Vert \bm{w} \Vert_1 + J(\bm{w}; \bm{X}, \bm{y}) \nonumber \\
\nabla_{\bm{w}} \tilde{J}(\bm{w}; \bm{X}, \bm{y}) &= \alpha \ \text{sign}(\bm{w}) + \nabla_{\bm{w}} J(\bm{w}; \bm{X}, \bm{y}) \nonumber
\end{align}
Note that the regularisation contribution to gradient is a constant factor with sign equal to $\text{sign}(w_i)$. Minimum of $\tilde{J}$ occurs at where
\begin{equation}
\nabla_{\bm{w}} \tilde{J}(\bm{w}) = \bm{H} (\bm{w} - \bm{w}^*) \nonumber
\end{equation}
Assuming the Hessian is diagonal, $\bm{H} = \text{diag}([H_{1,1}, \ldots, H_{n,n}])$, where each $H_{i,i} > 0$. The quadratic approximation of regularised objective function is then
\begin{align}
\tilde{J}(\bm{w}; \bm{X}, \bm{y}) &= J(\bm{w}^*; \bm{X}, \bm{y}) + \sum_i \left[ \frac{1}{2} H_{i,i} (\bm{w}_i - \bm{w}_i^*)^2 + \alpha \abs{w_i} \right] \nonumber \\
w_i &= \text{sign}(w_i^*) \max \left\{ \abs{w_i^*} - \frac{\alpha}{H_{i,i}}, 0 \right\} \nonumber
\end{align}
For the case where $w_i^* > 0$ for all $i$, then
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item if $w_i^* \leq \frac{\alpha}{H_{i,i}}$, the optimal value is $w_i = 0$ as contribution of $J(\bm{w}; \bm{X}, \bm{y})$ to regularised objective $\tilde{J}(\bm{w}; \bm{X}, \bm{y})$ is overwhelmed in direction $i$ by $L^1$ regularisation which pushes $w_i$ to zero.
\item if $w_i^* > \frac{\alpha}{H_{i,i}}$, regularisation shifts optimal value of $w_i$ in the direction by $\frac{\alpha}{H_{i,i}}$.
\end{enumerate}
For the case where $w_i^* < 0$, this happens similarly, but with $L^1$ penalty decreasing $w_i$ by $\frac{\alpha}{H_{i,i}}$, with min value $0$.\\
Note that $L^1$ produces a more sparse solution, which is a feature selection mechanism.
\end{definition}

\begin{remark} \hlt{Norm Penalties as Constrained Optimisation}\\
Let cost function regularised by parameter norm penalty be
\begin{equation}
\tilde{J}(\bm{\theta}; \bm{X}, \bm{y}) = J(\bm{\theta}; \bm{X}, \bm{y}) + \alpha \Omega(\bm{\theta}) \nonumber
\end{equation}
Construct a generalised generalised Lagrange function,
\begin{equation}
\mathcal{L}(\bm{\theta}; \bm{X}, \bm{y}) = J(\bm{\theta}; \bm{X}, \bm{y}) + \alpha (\Omega(\bm{\theta}) - k) \nonumber
\end{equation}
The solution to the constrained problem is then
\begin{equation}
\bm{\theta}^* = \arg \min_{\bm{\theta}} \max_{\alpha, \alpha \geq 0} \mathcal{L} (\bm{\theta}, \alpha) \nonumber
\end{equation}
Note that optimal value $\alpha^*$ will shrink $\Omega(\bm{\theta})$, but not such that it is less than $k$.\\
Fixing $\alpha^*$, the problem is then a function of $\bm{\theta}$,
\begin{equation}
\bm{\theta}^* = \arg \min_{\bm{\theta}} \mathcal{L} (\bm{\theta}, \alpha^*) = \arg \min_{\bm{\theta}} J(\bm{\theta}; \bm{X}, \bm{y}) + \alpha^* \Omega (\bm{\theta}) \nonumber
\end{equation}
This is the regularised training problem of minimising $\tilde{J}$. The parameter norm penalty is imposing a constraint on the weights. If explicit constraints are to be used rather than penalties, use stochastic gradient descent on $J(\bm{\theta})$, then take the projected $\bm{\theta}$ to the nearest point that satisfies $\Omega(\bm{\theta}) < k$. This method is used if $k$ is predefined by user, and time is not to be spent on searching for $\alpha$ value that corresponds to this $k$.\\
Explicit constraints and re-projection work better in circumstances where non-convex optimisation is involved, and this may avoid getting stuck in local minima. Explicit constraints also impose stability, and together with high learning rate allows rapid exploration of parameter space.
\end{remark}

\begin{remark} \hlt{Dataset Augmentation}\\
Effective technique for object classification. Operations such as translation of images by few pixels in each direction can greatly improved generalisation, even if the model has been designed to be partially translation invariant using convolution and pooling techniques. Rotation and scaling of images are also quite effective.\\
For speech recognition tasks, noice injection will improve performance. To improve robustness, train models with random noise applied to inputs. Noise injection also works when applied to the hidden units (data augmentation at multiple levels of abstraction). Enabling dropout constructs new inputs by multiplying by noise.
\end{remark}

\begin{remark} \hlt{Noise Robustness}\\
For some models, addition of noise with infinitesimal variance at input of model is equivalent to imposing penalty on norm of the weights. Generally, noise injection can be much more powerful than simply shrinking the parameters, especially when noise is added to hidden units.\\
Noise may also be added to weights, which is primarily used in recurrent neural networks. This is a stochastic implementation of Bayesian inference over the weights (model weights are uncertain and representable via a probability distribution that reflects this uncertainty).\\
Adding noise to weights may also result in more stability in function to be learned. Let the cost function be 
\begin{align}
J = \mathbb{E}_{p(x,y)}[(\hat{y}(\bm{x}) - y)^2] \nonumber
\end{align}
where $\hat{y}(\bm{x})$ is function to be trained that maps set of features $\bm{x}$ to a scalar using least-squares cost function $J$.\\
Assume each input presentation include a random perturbation $\epsilon_{\bm{W}} \sim \mathcal{N}(\bm{\epsilon}; \bm{0}, \eta \bm{I})$ in the network weight of a standard $l$-layer MLP. Let the perturbed model be $\hat{y}_{\bm{\epsilon}_{\bm{W}}}(\bm{x})$. With injection o noise, objective function becomes
\begin{align}
\tilde{J}_{\bm{W}} &= \mathbb{E}_{p(\bm{x}, y, \bm{\epsilon}_{\bm{W}})}\left[(\hat{y}_{\bm{\epsilon}_{\bm{W}}}(\bm{x}) - y)^2 \right] = \mathbb{E}_{p(\bm{x}, y, \bm{\epsilon}_{\bm{W}})} \left[\hat{y}^2_{\bm{\epsilon}_{\bm{W}}}(\bm{x}) - 2 y \hat{y}_{\bm{\epsilon}_{\bm{W}}} + y^2 \right] \nonumber
\end{align}
For small $\eta$, the minimisation of $J$ with added noise (with covariance $\eta \bm{I}$) is equivalent to minimisation of $J$ with additional regularisation term $\eta \mathbb{E}_{p(\bm{x}), y} [\norm{\nabla_{\bm{W}} \hat{y}(\bm{x})}^2]$. This encourages parameters to go to regions of parameter space where small perturbations of weights have relatively small influence on output.
\end{remark}

\begin{remark} \hlt{Label Smoothing}\\
Datasets may have some mistake in $y$ labels. Regularise a model based on softmax with $k$ output values by replacing hard $0$ and $1$ classification targets with targets of $\frac{\epsilon}{k-1}$ and $1-\epsilon$ respectively, assuming that the training set label $y$ is correct with probability $1- \epsilon$. Cross-entropy can then be used on these soft targets.
\end{remark}

\begin{remark} \hlt{Semi-Supervised Learning}\\
Unlabelled examples $P(\bm{x})$ and $P(\bm{x}, \bm{y})$ are used to estimate $P(\bm{y} \ | \ \bm{x})$ or predict $\bm{y}$ from $\bm{x}$.\\
Construct a generative model of either $P(\bm{x})$ or $P(\bm{x}, \bm{y})$ which shares parameters with a discriminative model of $P(\bm{y} \ | \ \bm{x})$. The supervised criterion $\log P(\bm{y} \ | \ \bm{x})$ may then be traded with the unsupervised or generative one ($\log P(\bm{x})$ or $\log P(\bm{x}, \bm{y})$). The generative criterion then expresses a particular form of prior belief about the solution to the supervised learning problem (structure of $P(\bm{x})$ is connected to structure of $P(\bm{y} \ | \ \bm{x})$ such that is is captured by the shared parametrisation).
\end{remark}

\begin{remark} \hlt{Multi-Task Learning}\\
Improve generalisations by pooling examples (soft constraints imposed on parameters) from several tasks.\\
Different supervised tasks (predicting $\bm{y}^{(i)}$ given $\bm{x}$) share the same input $\bm{x}$, as well as some intermediate-level representation $\bm{h}^{(\text{shared})}$ capturing a common pool of factors. Model can be divided in two parts:
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Task-specific parameters
\item Generic parameters, shared across all the tasks
\end{enumerate}
Improved generalisation and associated error bounds can be achieved due to shared parameters, but this only happens if some assumptions about statistical relationship between different tasks are valid.
\end{remark}

\begin{remark} \hlt{Early Stopping}\\
To obtain a model with better validation set error by returning to the parameter setting at the point in time with lowest validation set error. Model validation is done in parallel to model training.\\
Early stopping has the effect of restricting the optimisation procedure to a relatively small volume of parameter space in neighbourhood of initial parameter value $\bm{\theta}_o$. Assume $\tau$ optimisation steps are taken with learning rate $\epsilon$. The product $\epsilon \tau$ is a measure of effective capacity. Assuming gradient is bounded, restricting $\epsilon \tau$ limits the volume of parameter space reachable from $\bm{\theta}_o$, hence $\epsilon \tau$ behaves as the reciprocal of weight decay coefficients.
\end{remark}

\begin{breakablealgorithm}
\caption{Early Stopping Algorithm}
\begin{algorithmic}
\Require \\
Number of steps $n$ between evaluations\\
Patience $p$, the number of times to observe worsening validation error before stopping\\
Initial parameters $\bm{\theta}_o$\\
    
\State $\bm{\theta} \leftarrow \bm{\theta}_o$, \ \ $i \leftarrow 0$, \ \ $j \leftarrow 0$, \ \ $v \leftarrow \infty$, \ \ $\bm{\theta}^{*} \leftarrow \bm{\theta}$, \ \ $i^* \leftarrow i$
\While{$j < p$}
\State Update $\bm{\theta}$ by running training algorithm for $n$ steps
\State $i \leftarrow i + n$, \ \ $v' \leftarrow \text{ValidationSetError}(\bm{\theta})$
\If{v' < v}
\State $j \leftarrow 0$, \ \ $\bm{\theta}^{*} \leftarrow \bm{\theta}$, \ \ $i^* \leftarrow i$, \ \ $v \leftarrow v'$
\EndIf
\State $j \leftarrow j+1$
\EndWhile
\State Best parameters are $\bm{\theta}^*$, best number of training steps is $i^*$

\end{algorithmic}
\end{breakablealgorithm}

\begin{remark} \hlt{Parameter Tying and Parameter Sharing}\\
Used to express prior knowledge on suitable values of model parameters (precise values are not known, but there should be some dependencies between parameters). To regularise parameters to be close, may use:
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Parameter norm penalty: assume model $A$ with parameters $\bm{w}^{(A)}$ and model $B$ with parameters $\bm{w}^{(B)}$. Two models map input to two different but related outputs $\hat{y}^{(A)} = f(\bm{w}^{(A)}, \bm{x})$ and $\hat{y}^{(B)} = g(\bm{w}^{(B)}, \bm{x})$.\\
The model parameters $w_{i}^{(A)}$ should be close to  $w_{i}^{(B)}$ $\forall i$. Using regularisation, a parameter norm penalty may be imposed, such as using $L^2$ penalty to get $\Omega(\bm{w}^{(A)}, \bm{w}^{(B)}) = \norm{\bm{w}^{(A)} - \bm{w}^{(B)}}^2_2$.
\item Parameter sharing: using constraints. Only a subset of the parameters (input set) need to be stored in memory. May lead to significant reduction in memory footprint of the model (i.e., in CCNs)
\end{enumerate}
\end{remark}

\begin{remark} \hlt{Sparse Representations}\\
Representation sparsity is where elements of the representation are zero, accomplished by same sort of mechanisms used in parameter regularisation.\\
Given a linear regression $\bm{y} = \bm{B} \bm{h}$, where $\bm{y} \in \R^m, \bm{B} \in \R^{m \times n}, \bm{h} \in \R^n$, and $\bm{h}$ is the representation of data $\bm{x}$, add to loss function $J$ a norm penalty on the representation ($\Omega(\bm{h}))$):
\begin{align}
\tilde{J}(\bm{\theta}; \bm{X}, \bm{y}) = J(\bm{\theta}; \bm{X}, \bm{y}) + \alpha \Omega (\bm{h}) \nonumber
\end{align}
where $\alpha \in [0, \infty)$ weights relative contribution of norm penalty, with larger values of $\alpha$ for more regularisation.\\
$L^1$ penalty on elements of representation induces sparsity, $\Omega(\bm{h}) = \norm{\bm{h}}_1 = \sum_i \abs{h_i}$. For representations with elements constrained to lie on unit interval, may use Student-$t$ prior or KL divergence penalties.\\
Other approaches such as hard constraint on activation values may also work. \hlt{Orthogonal matching pursuit} encodes an input $\bm{x}$ with representation $\bm{h}$ that solves the constrained optimisation problem
\begin{align}
\arg \min_{\bm{h}, \norm{\bm{h}}_0 < k} \norm{\bm{x} - \bm{W} \bm{h}}^2 \nonumber
\end{align}
where $\norm{\bm{h}}_0$ is the number of non-zero entries of $\bm{h}$. If $\bm{W}$ is constrained to be orthogonal, this can be solved efficiently with OMP-$k$ method, where $k$ is number of non-zero features allowed.
\end{remark}

\begin{remark} \hlt{Bootstrap Aggregating (Bagging)}\\
Construct $k$ different datasets, each with same number of examples as original dataset, but constructed by sampling with replacement from original dataset. Each dataset may miss some examples but also contain duplicate examples. Each model $i$ is then trained on the dataset $i$. Differences in random initialisation, random selection of mini-batches, differences in hyperparameters, or different outcomes of non-deterministic implementation of neural networks are enough to allow each model to make partially independent errors.
\end{remark}

\begin{remark} \hlt{Boosting}\\
Construct $k$ different datasets from original dataset. Using $i$th dataset, train $i$th weak learner. Test the $i$th weak learner with validation dataset, and each datapoint with wrong prediction is sent to $(i+1)$th dataset. The $(i+1)$th dataset is then used to train the $(i+1)$th weak learner. Continue until reaching the total number of subsets for the total prediction. 
\end{remark}

\begin{remark} \hlt{Dropout}\\
Dropout trains the ensemble consisting of all sub-networks that can be formed by removing non-output units from the base network by multiplying its output value by zero. To train a neural network with dropout, use minibatch-based learning algorithms (i.e., Stochastic Gradient Descent). Each time an example is loaded into minibatch, randomly (and independently) sample a different binary mask $\bm{\mu}$ to apply to all of input and hidden units in the network, where the probability of sampling a mask value of one is the dropout rate.\\
Let $J(\bm{\theta}, \bm{\mu})$ be cost of model defined by parameters $\bm{\theta}$ and mask $\bm{\mu}$. Dropout training is minimising $\mathbb{E}_{\bm{\mu}}J(\bm{\theta}, \bm{\mu})$. Each model inherits different subset of parameters from parent neural network, and a tiny fraction of possible sub-networks are trained for a single step. Parameter sharing allows sub-networks to arrive at good parameter settings. Each sub-network given by $\bm{\mu}$ defines a probability distribution $p(y \ | \ \bm{x}, \bm{\mu})$. The arithmetic mean over all masks is then given by
\begin{align}
\sum\limits_{\bm{\mu}} p(\bm{\mu}) p(y \ | \ \bm{x}, \bm{\mu}) \nonumber
\end{align}
where $p(\bm{\mu})$ is the distribution used to sample $\bm{\mu}$ during training.\\
Using dropout as a regularisation technique reduces effective capacity of a model. To offset this, increase the size of the model. Typically optimal validation error is much lower when using dropout, but this comes at cost of larger model and more iterations of training.\\
For very large datasets, costs of using dropout and larger models may outweigh benefits of regularisation. For sparse datasets (extremely few labelled training examples available), Bayesian neural networks outperform dropout. When additional unlabelled data is available, unsupervised learning may gain advantage over dropout.
\end{remark}

\begin{remark} \hlt{Adversarial Training}\\
Adversarial example may be constructed by optimisation to search for an input $\bm{x}'$ near $\bm{x}$ such that the model output is very different at $\bm{x}'$.\\
The primary cause is excessive linearity in some neural networks. Changing input by $\epsilon$ causes linear function with weights $\bm{w}$ to change by as much as $\epsilon \norm{\bm{w}}_1$, which can be very large if $\bm{w}$ is high-dimensional.\\
Adversarial examples assists in semi-supervised learning. At point $\bm{x}$ that is not associated with a label, model assigns label $\hat{y}$ (may not be true label). Seek adversarial example $\bm{x}'$ that causes classifier to output label $y' \neq \hat{y}$. Classifier may then be trained to assign same label to $\bm{x}$ and $\bm{x}'$, learning function robust to small changes.
\end{remark}

\begin{remark} \hlt{Tangent Distance Algorithm}\\
Non-parametric nearest-neighbour algorithm, where examples on the same manifold share the same category. The nearest-neighbour distance between points $\bm{x}_1$ and $\bm{x}_2$ is the distance between manifolds $M_1$ and $M_2$. Computationally cheap alternative is to approximate $M_i$ by its tangent plane at $\bm{x}_i$ and measure the distance between two tangents, or between a tangent plane and a point.
\end{remark}

\begin{remark} \hlt{Tangent Propagation Algorithm}\\
Trains neural net classifier with extra penalty to make each output $f(\bm{x})$ of neural net locally invariant to factors of variation, which correspond to movement along manifold which examples from same class concentrate. Local invariance is achieved by requiring $\nabla_{\bm{\x}}f(\bm{x})$ to be orthogonal to known manifold tangent vectors $\bm{v}^{(i)}$ at $\bm{x}$, or equivalently the directional derivative of $f$ at $\bm{x}$ in directions $\bm{v}^{(i)}$ be small by adding regularisation penalty:
\begin{align}
\Omega(f) = \sum\limits_i \left( \left( \nabla_{\bm{x}} f(\bm{x}) \right)^T \bm{v}^{(i)} \right)^2 \nonumber
\end{align}
Tangent vectors are derived a priori from effect of transformations (i.e., translation, rotation, scaling).\\
Note this regularises model to resist infinitesimal perturbation, and the infinitesimal approach poses difficulties for models based on rectified linear units.\\
Data augmentation is the non-infinitesimal version of Tangent Propagation Algorithm, where network is explicitly trained to correctly classify distinct inputs created from more than infinitesimal transformation.
\end{remark}

\begin{remark} \hlt{Double Back-propagation Algorithm}\\
Algorithm regularises Jacobian to be small. Require model to be invariant to all directions of change in input so long as the change is small. Adversarial training is the non-infinitesimal version of Double Back-propagation Algorithm, where inputs were found near original inputs and model is trained to produce same input.
\end{remark}

\begin{remark} \hlt{Manifold Tangent Classifier}\\
Eliminates need to know tangent vectors a priori. Auto-encoders may estimate manifold tangent vectors, and the classifier makes use of this technique to avoid requiring user-specified tangent vectors. The steps are:
\begin{enumerate}[label=\roman*.]
\setlength{\itemsep}{0pt}
\item Use auto-encoder to learn manifold structure by unsupervised learning
\item Use these tangents to regularise a neural net classifier as in Tangent Propagation Algorithm.
\end{enumerate}
\end{remark}